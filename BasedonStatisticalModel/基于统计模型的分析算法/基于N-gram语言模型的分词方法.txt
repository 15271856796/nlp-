什么是语言模型:简单来说就是判断一句话是不是说的人话(人正常表达的一句话),看看是不是一句通顺的话
    比如以下的三句话:
                今天天气很好,我们去圆明园玩吧
                今天很好天气,我们去圆明园玩吧
                今天我们天气,很好去圆明园玩吧
    如何判断一个句子是否合理,很容易想到了一种很好的统计模型来解决上述问题,只需要看它在所有的
    句子中出现的概率就行了,第一个句子出现的概率大概是80%,第二个句子出现的概率大概是50%,第三个句子出现的概率大概
    是20%,第一个句子出现的可能性最大,因此这个句子是最为合理的,这就是统计语言模型

那么如何计算一个句子出现的概率呢,我们可以把有史以来人类说过的话都统计一遍,这样就
能很方便的计算概率了,然而,你们都知道这条路走不通
假设想知道S在文本中出现的可能性,也就是说数学上所说的S的概率,既然S=w1,w2,.....wn 那么不妨把S展开表示,
        P(S)=P(w1,w2,w3,.....wn)
        P(w1,w2,w3,....,wn)=P(w1)P(w2/w1)P(w3/w1,w2)......P(wn/w1,w2,w3,...wn-1)
计算P(w1)很容易,P(w2/w1)也还能算出来,但是p(w3/w1,w2)已经非常难以设计了,所以我们引出了偷懒的马尔科夫
假设上面的n不取很长,而只取两个,那么就可以大大的减少计算量,即在此时,假设一个词数显的概率只和它前面的wi-1有关
这种假设称为1阶马尔科夫假设,现在概率就变得很简单了:
        P(w1,w2,w3,....,wn)=P(w1)P(w2/w1)P(w3/w2).....P(wn/wn-1)
所以现在的问题就变成了要估计条件概率P(wi/wi-1),根据它的定义就是
        P(wi/wi-1)=P(wi-1,wi)/P(wi-1)
当样本量很大的时候,基于大数定律,一个短语或者词语出现的概率可以用其频率来表示的时候,即
        p(wi,wi-1)=count(wi,wi-1)/count(*)
        p(wi-1)=count(wi-1)/count(*)
        p(wi/wi-1)=count(wi-1,wi)/count(wi-1)


那么是怎么把N-gram 用于分词的呢?
    由于歧义的存在,一段文本存在多种可能的切分结果(切分路径),
    正向最大匹配(FMM)和逆向最大匹配(BMM)使用机械规划的方法选择最优路径,而N-gram语言
    模型的分词方法是利用统计信息找出一条概率最大的路径(只要可以独立成词的就分一次)
    比如武汉市长江大桥 可以有很多分法 (武汉/市/长江/大桥)(武汉/市/长江大桥)(武汉市/长江/大桥)
    还有(武汉/市长/江/大桥)(武汉/市长/江大桥)等等 我们需要找到概率最大的一种P(W)






